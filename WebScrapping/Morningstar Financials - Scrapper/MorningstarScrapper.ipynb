{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Click on the code cell below and then click the Run button above ^^ to run the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neccessary installations and imports  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium\n",
    "!pip install bs4\n",
    "!pip install webdriver_manager\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from time import sleep\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = pd.read_excel('morningstar_links.xlsx')\n",
    "input_df.drop(['status', 'Web Open', 'Ticker'], inplace=True, axis=1)\n",
    "new_df = input_df[input_df['finding'].isnull()]\n",
    "new_df.drop(['finding'], inplace=True, axis=1)\n",
    "df = new_df[(new_df[\"Remark\"] == 'Key Ratio')]\n",
    "for (index_label, row_series) in df.iterrows():\n",
    "    var = (str(row_series.values))\n",
    "    url = (var[2:-14])\n",
    "    options = Options()\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install(), options=options)\n",
    "    driver.get(url)\n",
    "    time.sleep(10)\n",
    "    driver.find_element_by_xpath('/html/body/div[1]/div[3]/div[2]/div[1]/div/div/div[2]/div/a').click()    \n",
    "    time.sleep(10)\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_file_path=r'C:\\Temp\\Val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "\n",
    "input_df = pd.read_excel('morningstar_links.xlsx')\n",
    "input_df.drop(['status', 'Web Open', 'Ticker'], inplace=True, axis=1)\n",
    "new_df = input_df[input_df['finding'].isnull()]\n",
    "new_df.drop(['finding'], inplace=True, axis=1)\n",
    "df = new_df[(new_df[\"Remark\"] == 'Valuation')]\n",
    "\n",
    "df = new_df[(new_df[\"Remark\"] == 'Valuation')]\n",
    "for (index_label, row_series) in df.iterrows():\n",
    "    var = (str(row_series.values))\n",
    "    url = (var[2:-14])\n",
    "    options = Options()\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install(), options=options)\n",
    "    driver.get(url)\n",
    "    sleep(10)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html)\n",
    "    sleep(5)\n",
    "    driver.close()\n",
    "    cname = soup.find('div', {'class': 'mdc-security-header__inner'})\n",
    "    c_name = cname.span.text\n",
    "    c_name = c_name.replace('?', '-')\n",
    "    rdate = soup.find('div', {'class': 'mdc-security-header__rating-description'})\n",
    "    r_date = rdate.text[4:]\n",
    "    val = soup.find('div', {'class': 'sal-component-header'})\n",
    "    vals = val.text[0:-1]\n",
    "    val2 = soup.find('div', {'class': 'sal-component-disclosure'})\n",
    "    val2 = val2.text\n",
    "    header = soup.find_all(\"table\")[0].find(\"tr\")\n",
    "    data = []\n",
    "    list_header = []\n",
    "    for items in header:\n",
    "        try:\n",
    "            list_header.append(items.get_text())\n",
    "        except:\n",
    "            continue\n",
    "    HTML_data = soup.find_all(\"table\")[0].find_all(\"tr\")[1:]\n",
    "    for element in HTML_data:\n",
    "        sub_data = []\n",
    "        for sub_element in element:\n",
    "            try:\n",
    "                sub_data.append(sub_element.get_text().strip())\n",
    "            except:\n",
    "                continue\n",
    "        data.append(sub_data)\n",
    "    dataFrame = pd.DataFrame(data = data, columns = list_header)\n",
    "    dataFrame = dataFrame.replace('\\n', ' ', regex = True)\n",
    "    dataFrame.rename(columns={'\\n                      Calendar\\n                     ': 'Calendar', '\\n                      2011\\n                     ': '2011','\\n                      2012\\n                     ':'2012','\\n                      2013\\n                     ':'2013','\\n                      2014\\n                     ':'2014','\\n                      2015\\n                     ':'2015','\\n                      2016\\n                     ':'2016','\\n                      2017\\n                     ':'2017','\\n                      2018\\n                     ':'2018','\\n                      2019\\n                     ':'2019','\\n                      2020\\n                     ':'2020','\\n                      Current\\n                     ':'Current','\\n                      5-Yr\\n                     ':'5-Yr','\\n                      Index\\n                     ':'Index'}, inplace=True)\n",
    "    df = dataFrame.replace('––',' ', regex=True)\n",
    "    df1 ={'Calendar': ' ', '2011': ' ', '2012': ' ', '2013': ' ', '2014': ' ', '2015': ' ', '2016': ' ', '2017': ' ', '2018': ' ', '2019': ' ', '2020': ' ', 'Current': ' ', '5-Yr': ' ', 'Index': ' ', }\n",
    "    df2 ={'Calendar': c_name, '2011': ' ', '2012': ' ', '2013': ' ', '2014': ' ', '2015': ' ', '2016': ' ', '2017': ' ', '2018': ' ', '2019': ' ', '2020': ' ', 'Current': ' ', '5-Yr': ' ', 'Index': ' ', }\n",
    "    df3 ={'Calendar': r_date, '2011': ' ', '2012': ' ', '2013': ' ', '2014': ' ', '2015': ' ', '2016': ' ', '2017': ' ', '2018': ' ', '2019': ' ', '2020': ' ', 'Current': ' ', '5-Yr': ' ', 'Index': ' ', }\n",
    "    df4 ={'Calendar': val2, '2011': ' ', '2012': ' ', '2013': ' ', '2014': ' ', '2015': ' ', '2016': ' ', '2017': ' ', '2018': ' ', '2019': ' ', '2020': ' ', 'Current': ' ', '5-Yr': ' ', 'Index': ' ', }\n",
    "    df = df.append(df1, ignore_index = True)\n",
    "    df = df.append(df2, ignore_index = True)\n",
    "    df = df.append(df3, ignore_index = True)\n",
    "    df = df.append(df4, ignore_index = True)\n",
    "    df = df.replace('\\n',' ', regex=True)\n",
    "    df = df.replace('\\t',' ', regex=True)\n",
    "    df = df.replace('\\s+', ' ', regex=True)\n",
    "    \n",
    "    df.to_csv(os.path.join(saving_file_path, url[40:-10] + ' Val.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
