{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec16dc93-a0fb-4245-9c52-37d81b73e92a",
   "metadata": {},
   "source": [
    "# Chat Bot using Text Embeddings and comparing similarity with Cosine Similarity. \n",
    "\n",
    "# Suggesting relevant questions on the not answered questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44cf92e-5cf8-4e09-be1a-be304b717478",
   "metadata": {},
   "source": [
    "## Google Text Embedding Model: textembedding-gecko@001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7363bd66-d8f4-4975-8d9f-c419defb019f",
   "metadata": {},
   "source": [
    "### Look at the Chat-Bot_Text-Embedding-Gecko notebook for a detailed explaination \n",
    "\n",
    "#### Notebook link:\n",
    "##### https://github.com/Rizi2001/TechGallery/blob/main/Chat%20Bot%20TextEmbeddings-Gecko%40001%20Model%20Cosine%20Similarity/Chat-Bot_Text-Embedding-Gecko.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b586ea80-2b35-45e2-8b39-828c40d87cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-25 11:59:23.274763: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-11-25 11:59:24.769772: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from csv import DictWriter\n",
    "from sentence_transformers import util\n",
    "from vertexai.language_models import TextEmbeddingModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cc14932-2bc6-41be-a03e-4f1b7b7d5dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = ''#Path to your Google Cloud's Service Account Crediatials JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7113cba6-1e85-4ff4-84f4-59deb7ffee39",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_df = pd.read_csv(\"Question_Variations_Embeddings_Updated.csv\")\n",
    "embedding_df['Embedding'] = embedding_df['Embedding'].apply(ast.literal_eval)\n",
    "\n",
    "embedding_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bf79d94-bcb9-4f84-aa1b-eb3b213ed83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embedding_df)\n",
    "question_embeddings = embedding_df['Embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae2bde1e-fbea-4065-90c8-aebd5be2a8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36b794ee-defd-4290-bc35-5ae9c3e5d5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b78460e-afba-421f-b1a4-fb4525dda36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc if not token.is_punct])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1952521c-147a-4a0a-bf99-00ef24a2db7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_similar_question(user_question, question_embeddings_df):\n",
    "    user_question = preprocess_text(user_question)\n",
    "    \n",
    "    user_question_embedding = model.get_embeddings([user_question])\n",
    "    \n",
    "    user_question_embedding = [embedding.values for embedding in user_question_embedding]\n",
    "\n",
    "    sim_score = [util.pytorch_cos_sim(user_question_embedding, i).item() for i in question_embeddings]\n",
    "\n",
    "    threshhold = 0.83\n",
    "    \n",
    "    max_sim_score = max(sim_score)\n",
    "\n",
    "    #print(\"Maximum similarity Score: \", max_sim_score)\n",
    "\n",
    "    if max_sim_score >= threshhold:\n",
    "        sim_arg = sim_score.index(max_sim_score)\n",
    "\n",
    "        return embedding_df['Answer'][sim_arg]\n",
    "    else:\n",
    "        print(\"Sorry, I didn't understand your question, Are you trying to ask any of the below?\\n\\n\")\n",
    "\n",
    "        # sorted_indices = sorted(range(len(sim_score)), key=lambda k: sim_score[k], reverse=True)\n",
    "        # top_three_indices = sorted_indices[:3]\n",
    "\n",
    "        # for idx in top_three_indices:\n",
    "        #     suggested_question = embedding_df['Question'][idx]\n",
    "        #     print(f\"\\nAre you trying to ask: '{suggested_question}'?\")\n",
    "\n",
    "        sorted_indices = sorted(range(len(sim_score)), key=lambda k: sim_score[k], reverse=True)\n",
    "        top_three_indices = sorted_indices[:3]\n",
    "\n",
    "        print(\"Suggestions:\")\n",
    "        for i, idx in enumerate(top_three_indices, 1):\n",
    "            suggested_question = embedding_df['Question'][idx]\n",
    "            print(f\"{i}. {suggested_question}\")\n",
    "\n",
    "        try:\n",
    "            selected_option = int(input(\"Select a suggestion (1, 2, or 3): \"))\n",
    "            if 1 <= selected_option <= 3:\n",
    "                return embedding_df['Answer'][top_three_indices[selected_option - 1]]\n",
    "            else:\n",
    "                print(\"Invalid selection. Please try again.\")\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a number.\")\n",
    "\n",
    "        return None\n",
    "\n",
    "        # return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438aa018-d533-48fa-89c7-fe4ae574e3cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8a38c37-9bb1-4549-a174-5d240a17dc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Cirrus Chat. Ask your question, type \"exit\" to close this conversation\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hi! How can I assist you today?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " add patient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, I didn't understand your question, Are you trying to ask any of the below?\n",
      "\n",
      "\n",
      "Suggestions:\n",
      "1. How can I add a new patient to my practice?\n",
      "2. What steps should I take to set my patient as Inactive?\n",
      "3. How do I set my patient as Inactive?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Select a suggestion (1, 2, or 3):  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " To set your patient as Inactive take the following steps: 1.\tGo to the Patient module. 2.\tClick on the three dots right next to the Patient MRN 3.\tPatient context menu will be displayed in the tooltip. 4.\tClick on the Mark as Inactive option to mark your patient as inactive. \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " exit\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"Welcome to your Chat Bot. Ask your question, type \"exit\" to close this conversation\"\"\")\n",
    "while(1):\n",
    "    user_input = input()\n",
    "\n",
    "    if user_input == \"exit\":\n",
    "        break\n",
    "    \n",
    "    response = get_most_similar_question(user_input, embedding_df)\n",
    "\n",
    "    print(f\"\\n{response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a22ec4-75d8-4186-bc68-1f599c527fda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
